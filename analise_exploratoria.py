# -*- coding: utf-8 -*-
"""analise_exploratoria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfzj-pi3bDJW4PY3m-EtiVnvA7WzzLEk
"""

import pandas as pd
import numpy as np
from scipy.stats import f_oneway
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats import pearsonr, spearmanr
from itertools import combinations
import plotly.express as px
from scipy.stats import chi2_contingency
from sklearn.metrics import cohen_kappa_score
import matplotlib.pyplot as plt
import seaborn as sns
import math
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

def bivariada_cat_num(df, show_graphs=True, p_value_only=True):
    # Silenciar avisos de forma segura sem depender de importação do scipy.stats
    # Isso ignora avisos de amostras pequenas e divisões por zero comuns em cálculos estatísticos
    warnings.filterwarnings('ignore', message='.*SmallSampleWarning.*')
    warnings.filterwarnings('ignore', message='.*all input arrays have length 1.*')
    warnings.simplefilter(action='ignore', category=RuntimeWarning)

    # --- 1. MÓDULO INTERNO: EXPANSÃO DE DATAS ---
    df_analise = df.copy()
    cols_data = df_analise.select_dtypes(include=['datetime64']).columns.tolist()

    for col in df_analise.select_dtypes(include=['object']).columns:
        if any(keyword in col.lower() for keyword in ['data', 'date', 'vencimento']):
            try:
                df_analise[col] = pd.to_datetime(df_analise[col])
                cols_data.append(col)
            except:
                continue

    for col in set(cols_data):
        df_analise[f'{col}_Ano'] = df_analise[col].dt.year.astype(str)
        df_analise[f'{col}_Mes'] = df_analise[col].dt.strftime('%m/%Y')
        df_analise[f'{col}_Dia_Semana'] = df_analise[col].dt.day_name()

    # --- 2. IDENTIFICAÇÃO DE VARIÁVEIS ---
    num_vars = df_analise.select_dtypes(include=[np.number]).columns.tolist()
    cat_vars = df_analise.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

    ranking_global = []

    def get_eta_benchmark(val):
        if val <= 0.01: return "Pequena"
        elif val <= 0.06: return "Média"
        elif val <= 0.14: return "Grande"
        else: return "Muito Grande"

    # --- 3. PROCESSAMENTO ---
    for cat_var in cat_vars:
        valid_plots = []

        for num_var in num_vars:
            temp_df = df_analise[[cat_var, num_var]].dropna()

            # Grupos para ANOVA
            grupos_raw = [group[1] for group in temp_df.groupby(cat_var, observed=True)[num_var]]

            # Verificação manual para evitar o cálculo se os dados forem insuficientes
            if len(grupos_raw) < 2 or all(len(g) <= 1 for g in grupos_raw):
                continue

            try:
                _, p_anova = f_oneway(*grupos_raw)
            except:
                p_anova = 1.0

            # Se o p-valor for nulo ou inválido, tratamos como 1.0 (não significativo)
            if np.isnan(p_anova):
                p_anova = 1.0

            # FILTRO DE P-VALOR
            if p_value_only and p_anova > 0.05:
                continue

            # Cálculo Eta²
            overall_mean = temp_df[num_var].mean()
            ss_total = sum((temp_df[num_var] - overall_mean) ** 2)
            ss_between = sum(len(g) * (g.mean() - overall_mean) ** 2 for g in grupos_raw)
            eta2 = ss_between / ss_total if ss_total != 0 else 0

            ranking_global.append({
                'Variável Categórica': cat_var,
                'Variável Numérica': num_var,
                'Eta²': round(eta2, 4),
                'Força': get_eta_benchmark(eta2),
                'p-valor ANOVA': round(p_anova, 4)
            })
            valid_plots.append(num_var)

        # --- 4. GERAÇÃO DOS GRÁFICOS ---
        if show_graphs and valid_plots:
            cols = 4
            rows = (len(valid_plots) - 1) // cols + 1
            fig = make_subplots(rows=rows, cols=cols, subplot_titles=valid_plots,
                                vertical_spacing=0.15, horizontal_spacing=0.05)

            for idx, num_var in enumerate(valid_plots):
                row = idx // cols + 1
                col = idx % cols + 1
                fig.add_trace(go.Box(x=df_analise[cat_var], y=df_analise[num_var], name=num_var), row=row, col=col)

            fig.update_layout(height=350 * rows, width=1200,
                              title_text=f"Associações Significativas por: {cat_var}",
                              title_x=0.5, template="plotly_white", showlegend=False)

            fig.update_annotations(font_size=10)
            fig.update_xaxes(tickfont=dict(size=9))
            fig.update_yaxes(tickfont=dict(size=9))
            fig.show()

    # --- 5. RANKING FINAL ---
    if ranking_global:
        df_ranking = pd.DataFrame(ranking_global).sort_values(by='Eta²', ascending=False).reset_index(drop=True)
        print("\n" + "="*95)
        print(f"RANKING GERAL (Filtro p-valor <= 0.05: {p_value_only})")
        print("="*95)
        print(df_ranking.to_string(index=False))
        #return df_ranking
    else:
        print("\nNenhuma associação significativa encontrada.")

def bivariada_num_num(df, show_graphs=True, p_value_only=True):
    # 1. Selecionar apenas colunas numéricas
    numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()

    if len(numeric_vars) < 2:
        print("É necessário pelo menos duas variáveis numéricas para correlação.")
        return

    combos = list(combinations(numeric_vars, 2))
    ranking_correlacao = []

    # Função auxiliar para benchmark de correlação
    def get_benchmark(val):
        abs_val = abs(val)
        if abs_val <= 0.10: return "Muito Fraca"
        elif abs_val <= 0.29: return "Fraca"
        elif abs_val <= 0.49: return "Moderada"
        elif abs_val <= 0.69: return "Forte"
        else: return "Muito Forte"

    # 2. Processamento Estatístico
    for var1, var2 in combos:
        temp_df = df[[var1, var2]].dropna()
        if len(temp_df) < 5:
            continue

        # Pearson e Spearman
        corr_p, p_p = pearsonr(temp_df[var1], temp_df[var2])
        corr_s, p_s = spearmanr(temp_df[var1], temp_df[var2])

        # Filtro de p-valor (baseado no Pearson como critério principal ou ambos)
        if p_value_only and p_p > 0.05 and p_s > 0.05:
            continue

        ranking_correlacao.append({
            'Variável 1': var1,
            'Variável 2': var2,
            'Pearson (r)': round(corr_p, 3),
            'Força Pearson': get_benchmark(corr_p),
            'p-valor P.': round(p_p, 4),
            'Spearman (rho)': round(corr_s, 3),
            'Força Spearman': get_benchmark(corr_s),
            'p-valor S.': round(p_s, 4),
            'Abs_Pearson': abs(corr_p)
        })

    # 3. Geração de Gráficos
    if show_graphs and ranking_correlacao:
        num_plots = len(ranking_correlacao)
        cols = 4
        rows = (num_plots - 1) // cols + 1

        fig = make_subplots(
            rows=rows, cols=cols,
            subplot_titles=[f"{r['Variável 1']} vs {r['Variável 2']}" for r in ranking_correlacao],
            vertical_spacing=0.15, horizontal_spacing=0.08
        )

        for i, res in enumerate(ranking_correlacao):
            row = i // cols + 1
            col = i % cols + 1
            temp_plot = df[[res['Variável 1'], res['Variável 2']]].dropna()

            fig.add_trace(
                go.Scatter(x=temp_plot[res['Variável 2']], y=temp_plot[res['Variável 1']],
                           mode='markers', marker=dict(size=5, opacity=0.6)),
                row=row, col=col
            )
            fig.update_xaxes(tickfont=dict(size=8), row=row, col=col)
            fig.update_yaxes(tickfont=dict(size=8), row=row, col=col)

        fig.update_layout(height=350 * rows, width=1200, title_text="Correlações Quantitativas",
                          title_x=0.5, showlegend=False, template="plotly_white")
        fig.update_annotations(font_size=9)
        fig.show()

    # 4. Ranking Final
    if ranking_correlacao:
        df_ranking = pd.DataFrame(ranking_correlacao)
        df_ranking = df_ranking.sort_values(by='Abs_Pearson', ascending=False).drop(columns=['Abs_Pearson']).reset_index(drop=True)

        print("\n" + "="*110)
        print(f"RANKING DE CORRELAÇÃO (Filtro p-valor <= 0.05: {p_value_only})")
        print("="*110)
        print(df_ranking.to_string(index=False))
        #return df_ranking

def bivariada_cat_cat(df, show_graphs=True, normalize='none', handle_missing='drop', p_value_only=True):
    # Silenciar avisos de cálculos estatísticos irrelevantes
    warnings.filterwarnings('ignore', message='.*chi2_contingency.*')

    # --- 1. MÓDULO INTERNO: EXPANSÃO DE DATAS ---
    df_analise = df.copy()
    cols_data = df_analise.select_dtypes(include=['datetime64']).columns.tolist()

    # Detecta objetos que parecem datas pelo nome da coluna
    for col in df_analise.select_dtypes(include=['object']).columns:
        if any(kw in col.lower() for kw in ['data', 'date', 'vencimento', 'abertura']):
            try:
                df_analise[col] = pd.to_datetime(df_analise[col])
                cols_data.append(col)
            except:
                continue

    # Cria categorias temporais
    for col in set(cols_data):
        df_analise[f'{col}_Ano'] = df_analise[col].dt.year.astype(str)
        df_analise[f'{col}_Mes'] = df_analise[col].dt.strftime('%m/%Y')
        df_analise[f'{col}_Dia_Semana'] = df_analise[col].dt.day_name()

    # --- 2. IDENTIFICAÇÃO DE VARIÁVEIS CATEGÓRICAS ATUALIZADA ---
    cat_vars = df_analise.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

    if len(cat_vars) < 2:
        print("É necessário pelo menos duas variáveis categóricas para análise.")
        return

    ranking_quali = []
    combos = list(combinations(cat_vars, 2))

    # --- 3. PROCESSAMENTO ESTATÍSTICO ---
    for v1, v2 in combos:
        df_temp = df_analise[[v1, v2]].copy()

        if handle_missing == 'drop':
            df_temp = df_temp.dropna()
        else:
            df_temp[v1] = df_temp[v1].fillna('Desconhecido')
            df_temp[v2] = df_temp[v2].fillna('Desconhecido')

        if len(df_temp) < 5 or df_temp[v1].nunique() < 2 or df_temp[v2].nunique() < 2:
            continue

        # Qui-Quadrado e p-valor
        absolute_table = pd.crosstab(df_temp[v1], df_temp[v2])
        try:
            chi2, p_val, _, _ = chi2_contingency(absolute_table)
        except:
            p_val = 1.0

        # Filtro de p-valor
        if p_value_only and (np.isnan(p_val) or p_val > 0.05):
            continue

        # V de Cramér
        n = absolute_table.values.sum()
        r, k = absolute_table.shape
        v_cramer = np.sqrt(chi2 / (n * (min(r, k) - 1))) if n > 0 else 0

        # Benchmark de Força
        if v_cramer <= 0.10: interp = "Muito Fraca"
        elif v_cramer <= 0.19: interp = "Fraca"
        elif v_cramer <= 0.29: interp = "Moderada"
        elif v_cramer <= 0.49: interp = "Forte"
        else: interp = "Muito Forte"

        try:
            kappa = cohen_kappa_score(df_temp[v1], df_temp[v2])
        except:
            kappa = np.nan

        ranking_quali.append({
            'Variável 1': v1,
            'Variável 2': v2,
            'V de Cramér': round(v_cramer, 4),
            'Força': interp,
            'p-valor': round(p_val, 4),
            'Kappa': round(kappa, 4) if not np.isnan(kappa) else "N/A"
        })

        # --- 4. GERAÇÃO DE GRÁFICOS ---
        if show_graphs:
            plot_table = pd.crosstab(df_temp[v2], df_temp[v1],
                                     normalize=normalize if normalize != 'none' else False)

            # Limita o tamanho do heatmap se houver muitas categorias para evitar travamentos
            if df_temp[v1].nunique() <= 20 and df_temp[v2].nunique() <= 20:
                fig = px.imshow(
                    plot_table,
                    labels=dict(x=v1, y=v2, color='Frequência'),
                    text_auto=".2f" if normalize != 'none' else True,
                    aspect="auto",
                    color_continuous_scale="Viridis",
                    title=f"ASSOCIAÇÃO SIGNIFICATIVA: {v1} vs {v2}<br><sup>V de Cramér: {v_cramer:.2f} ({interp}) | p-valor: {p_val:.4f}</sup>"
                )

                fig.update_layout(width=600, height=450, margin=dict(t=100))
                fig.update_xaxes(tickfont=dict(size=9))
                fig.update_yaxes(tickfont=dict(size=9))
                fig.show()

    # --- 5. RANKING FINAL ---
    if ranking_quali:
        df_ranking = pd.DataFrame(ranking_quali).sort_values(by='V de Cramér', ascending=False).reset_index(drop=True)
        print("\n" + "="*95)
        print(f"RANKING QUALITATIVO (Incluindo Datas | Filtro p-valor <= 0.05: {p_value_only})")
        print("="*95)
        print(df_ranking.to_string(index=False))
        #return df_ranking
    else:
        print("\nNenhuma associação qualitativa significativa encontrada.")

def univariada(df):

    for nome_coluna in df.columns:
        coluna = df[nome_coluna]

        print("="*60)
        print(f"ANALISANDO VARIÁVEL: {nome_coluna}")
        print("="*60)

        # 1. TRATAMENTO PARA QUALITATIVAS (Object, Category ou Booleana)
        if coluna.dtype == 'object' or isinstance(coluna.dtype, pd.CategoricalDtype) or coluna.dtype == 'bool':
            # Tabela de frequência
            tabela = coluna.value_counts(dropna=False).reset_index()
            tabela.columns = [nome_coluna, 'frequencia_absoluta']

            tabela['frequencia_relativa'] = tabela['frequencia_absoluta'] / tabela['frequencia_absoluta'].sum()
            tabela['frequencia_acumulada'] = tabela['frequencia_relativa'].cumsum()

            print('TABELA DE FREQUÊNCIA')
            print(tabela.to_string(index=False))
            print(f'\nValores Nulos: {coluna.isna().sum()}')
            print('-' * 30)

            # Gráficos Qualitativos
            if coluna.nunique() > 3:
                print('GRÁFICO DE BARRAS')
                plt.figure(figsize=(8, 4))
                # Ordenando barras pela frequência
                sns.barplot(data=tabela, x='frequencia_relativa', y=nome_coluna, palette='viridis', edgecolor='black')
                plt.title(f'Distribuição: {nome_coluna}')
            else:
                print('GRÁFICO DE PIZZA')
                plt.figure(figsize=(4, 4))
                plt.pie(tabela['frequencia_absoluta'], labels=tabela[nome_coluna], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
                plt.title(f'Proporção: {nome_coluna}')

            plt.tight_layout()
            plt.show()

        # 2. TRATAMENTO PARA QUANTITATIVAS (Numéricas)
        else:
            print('MEDIDAS ESTATÍSTICAS')
            print(coluna.describe())
            print(f'\nValores Nulos: {coluna.isna().sum()}')
            print('-' * 30)

            print('HISTOGRAMA E BOXPLOT')
            # Regra de Sturges para Bins
            if len(coluna.dropna()) > 0:
                num_bins = 1 + int(math.log2(len(coluna.dropna())))

                fig, axes = plt.subplots(1, 2, figsize=(10, 4))

                # Histograma com KDE (opcional, mas ajuda a ver a forma)
                sns.histplot(x=coluna.dropna(), bins=num_bins, kde=True, ax=axes[0], color='skyblue')
                axes[0].set_title(f'Histograma: {nome_coluna}')

                # Boxplot
                sns.boxplot(y=coluna.dropna(), ax=axes[1], color='salmon')
                axes[1].set_title(f'Boxplot: {nome_coluna}')

                plt.tight_layout()
                plt.show()
            else:
                print("Variável numérica sem dados válidos para plotagem.")

        print("\n" + " " * 60 + "\n") # Espaço entre variáveis
