# -*- coding: utf-8 -*-
"""analise_exploratoria.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pfzj-pi3bDJW4PY3m-EtiVnvA7WzzLEk
"""

import pandas as pd
import numpy as np
from scipy.stats import f_oneway
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.stats import pearsonr, spearmanr
from itertools import combinations
import plotly.express as px
from scipy.stats import chi2_contingency
from sklearn.metrics import cohen_kappa_score
import matplotlib.pyplot as plt
import seaborn as sns
import math
from scipy import stats
from statsmodels.tsa.stattools import adfuller, acf
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

def bivariada_cat_num(df, show_graphs=True, p_value_only=True):
    # Silenciar avisos de forma segura sem depender de importa√ß√£o do scipy.stats
    # Isso ignora avisos de amostras pequenas e divis√µes por zero comuns em c√°lculos estat√≠sticos
    warnings.filterwarnings('ignore', message='.*SmallSampleWarning.*')
    warnings.filterwarnings('ignore', message='.*all input arrays have length 1.*')
    warnings.simplefilter(action='ignore', category=RuntimeWarning)

    # --- 1. M√ìDULO INTERNO: EXPANS√ÉO DE DATAS ---
    df_analise = df.copy()
    cols_data = df_analise.select_dtypes(include=['datetime64']).columns.tolist()

    for col in df_analise.select_dtypes(include=['object']).columns:
        if any(keyword in col.lower() for keyword in ['data', 'date', 'vencimento']):
            try:
                df_analise[col] = pd.to_datetime(df_analise[col])
                cols_data.append(col)
            except:
                continue

    for col in set(cols_data):
        df_analise[f'{col}_Ano'] = df_analise[col].dt.year.astype(str)
        df_analise[f'{col}_Mes'] = df_analise[col].dt.strftime('%m/%Y')
        df_analise[f'{col}_Dia_Semana'] = df_analise[col].dt.day_name()

    # --- 2. IDENTIFICA√á√ÉO DE VARI√ÅVEIS ---
    num_vars = df_analise.select_dtypes(include=[np.number]).columns.tolist()
    cat_vars = df_analise.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

    ranking_global = []

    def get_eta_benchmark(val):
        if val <= 0.01: return "Pequena"
        elif val <= 0.06: return "M√©dia"
        elif val <= 0.14: return "Grande"
        else: return "Muito Grande"

    # --- 3. PROCESSAMENTO ---
    for cat_var in cat_vars:
        valid_plots = []

        for num_var in num_vars:
            temp_df = df_analise[[cat_var, num_var]].dropna()

            # Grupos para ANOVA
            grupos_raw = [group[1] for group in temp_df.groupby(cat_var, observed=True)[num_var]]

            # Verifica√ß√£o manual para evitar o c√°lculo se os dados forem insuficientes
            if len(grupos_raw) < 2 or all(len(g) <= 1 for g in grupos_raw):
                continue

            try:
                _, p_anova = f_oneway(*grupos_raw)
            except:
                p_anova = 1.0

            # Se o p-valor for nulo ou inv√°lido, tratamos como 1.0 (n√£o significativo)
            if np.isnan(p_anova):
                p_anova = 1.0

            # FILTRO DE P-VALOR
            if p_value_only and p_anova > 0.05:
                continue

            # C√°lculo Eta¬≤
            overall_mean = temp_df[num_var].mean()
            ss_total = sum((temp_df[num_var] - overall_mean) ** 2)
            ss_between = sum(len(g) * (g.mean() - overall_mean) ** 2 for g in grupos_raw)
            eta2 = ss_between / ss_total if ss_total != 0 else 0

            ranking_global.append({
                'Vari√°vel Categ√≥rica': cat_var,
                'Vari√°vel Num√©rica': num_var,
                'Eta¬≤': round(eta2, 4),
                'For√ßa': get_eta_benchmark(eta2),
                'p-valor ANOVA': round(p_anova, 4)
            })
            valid_plots.append(num_var)

        # --- 4. GERA√á√ÉO DOS GR√ÅFICOS ---
        if show_graphs and valid_plots:
            cols = 4
            rows = (len(valid_plots) - 1) // cols + 1
            fig = make_subplots(rows=rows, cols=cols, subplot_titles=valid_plots,
                                vertical_spacing=0.15, horizontal_spacing=0.05)

            for idx, num_var in enumerate(valid_plots):
                row = idx // cols + 1
                col = idx % cols + 1
                fig.add_trace(go.Box(x=df_analise[cat_var], y=df_analise[num_var], name=num_var), row=row, col=col)

            fig.update_layout(height=350 * rows, width=1200,
                              title_text=f"Associa√ß√µes Significativas por: {cat_var}",
                              title_x=0.5, template="plotly_white", showlegend=False)

            fig.update_annotations(font_size=10)
            fig.update_xaxes(tickfont=dict(size=9))
            fig.update_yaxes(tickfont=dict(size=9))
            fig.show()

    # --- 5. RANKING FINAL ---
    if ranking_global:
        df_ranking = pd.DataFrame(ranking_global).sort_values(by='Eta¬≤', ascending=False).reset_index(drop=True)
        print("\n" + "="*95)
        print(f"RANKING GERAL (Filtro p-valor <= 0.05: {p_value_only})")
        print("="*95)
        print(df_ranking.to_string(index=False))
        #return df_ranking
    else:
        print("\nNenhuma associa√ß√£o significativa encontrada.")

def bivariada_num_num(df, show_graphs=True, p_value_only=True):
    # 1. Selecionar apenas colunas num√©ricas
    numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist()

    if len(numeric_vars) < 2:
        print("√â necess√°rio pelo menos duas vari√°veis num√©ricas para correla√ß√£o.")
        return

    combos = list(combinations(numeric_vars, 2))
    ranking_correlacao = []

    # Fun√ß√£o auxiliar para benchmark de correla√ß√£o
    def get_benchmark(val):
        abs_val = abs(val)
        if abs_val <= 0.10: return "Muito Fraca"
        elif abs_val <= 0.29: return "Fraca"
        elif abs_val <= 0.49: return "Moderada"
        elif abs_val <= 0.69: return "Forte"
        else: return "Muito Forte"

    # 2. Processamento Estat√≠stico
    for var1, var2 in combos:
        temp_df = df[[var1, var2]].dropna()
        if len(temp_df) < 5:
            continue

        # Pearson e Spearman
        corr_p, p_p = pearsonr(temp_df[var1], temp_df[var2])
        corr_s, p_s = spearmanr(temp_df[var1], temp_df[var2])

        # Filtro de p-valor (baseado no Pearson como crit√©rio principal ou ambos)
        if p_value_only and p_p > 0.05 and p_s > 0.05:
            continue

        ranking_correlacao.append({
            'Vari√°vel 1': var1,
            'Vari√°vel 2': var2,
            'Pearson (r)': round(corr_p, 3),
            'For√ßa Pearson': get_benchmark(corr_p),
            'p-valor P.': round(p_p, 4),
            'Spearman (rho)': round(corr_s, 3),
            'For√ßa Spearman': get_benchmark(corr_s),
            'p-valor S.': round(p_s, 4),
            'Abs_Pearson': abs(corr_p)
        })

    # 3. Gera√ß√£o de Gr√°ficos
    if show_graphs and ranking_correlacao:
        num_plots = len(ranking_correlacao)
        cols = 4
        rows = (num_plots - 1) // cols + 1

        fig = make_subplots(
            rows=rows, cols=cols,
            subplot_titles=[f"{r['Vari√°vel 1']} vs {r['Vari√°vel 2']}" for r in ranking_correlacao],
            vertical_spacing=0.15, horizontal_spacing=0.08
        )

        for i, res in enumerate(ranking_correlacao):
            row = i // cols + 1
            col = i % cols + 1
            temp_plot = df[[res['Vari√°vel 1'], res['Vari√°vel 2']]].dropna()

            fig.add_trace(
                go.Scatter(x=temp_plot[res['Vari√°vel 2']], y=temp_plot[res['Vari√°vel 1']],
                           mode='markers', marker=dict(size=5, opacity=0.6)),
                row=row, col=col
            )
            fig.update_xaxes(tickfont=dict(size=8), row=row, col=col)
            fig.update_yaxes(tickfont=dict(size=8), row=row, col=col)

        fig.update_layout(height=350 * rows, width=1200, title_text="Correla√ß√µes Quantitativas",
                          title_x=0.5, showlegend=False, template="plotly_white")
        fig.update_annotations(font_size=9)
        fig.show()

    # 4. Ranking Final
    if ranking_correlacao:
        df_ranking = pd.DataFrame(ranking_correlacao)
        df_ranking = df_ranking.sort_values(by='Abs_Pearson', ascending=False).drop(columns=['Abs_Pearson']).reset_index(drop=True)

        print("\n" + "="*110)
        print(f"RANKING DE CORRELA√á√ÉO (Filtro p-valor <= 0.05: {p_value_only})")
        print("="*110)
        print(df_ranking.to_string(index=False))
        #return df_ranking

def bivariada_cat_cat(df, show_graphs=True, normalize='none', handle_missing='drop', p_value_only=True):
    # Silenciar avisos de c√°lculos estat√≠sticos irrelevantes
    warnings.filterwarnings('ignore', message='.*chi2_contingency.*')

    # --- 1. M√ìDULO INTERNO: EXPANS√ÉO DE DATAS ---
    df_analise = df.copy()
    cols_data = df_analise.select_dtypes(include=['datetime64']).columns.tolist()

    # Detecta objetos que parecem datas pelo nome da coluna
    for col in df_analise.select_dtypes(include=['object']).columns:
        if any(kw in col.lower() for kw in ['data', 'date', 'vencimento', 'abertura']):
            try:
                df_analise[col] = pd.to_datetime(df_analise[col])
                cols_data.append(col)
            except:
                continue

    # Cria categorias temporais
    for col in set(cols_data):
        df_analise[f'{col}_Ano'] = df_analise[col].dt.year.astype(str)
        df_analise[f'{col}_Mes'] = df_analise[col].dt.strftime('%m/%Y')
        df_analise[f'{col}_Dia_Semana'] = df_analise[col].dt.day_name()

    # --- 2. IDENTIFICA√á√ÉO DE VARI√ÅVEIS CATEG√ìRICAS ATUALIZADA ---
    cat_vars = df_analise.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()

    if len(cat_vars) < 2:
        print("√â necess√°rio pelo menos duas vari√°veis categ√≥ricas para an√°lise.")
        return

    ranking_quali = []
    combos = list(combinations(cat_vars, 2))

    # --- 3. PROCESSAMENTO ESTAT√çSTICO ---
    for v1, v2 in combos:
        df_temp = df_analise[[v1, v2]].copy()

        if handle_missing == 'drop':
            df_temp = df_temp.dropna()
        else:
            df_temp[v1] = df_temp[v1].fillna('Desconhecido')
            df_temp[v2] = df_temp[v2].fillna('Desconhecido')

        if len(df_temp) < 5 or df_temp[v1].nunique() < 2 or df_temp[v2].nunique() < 2:
            continue

        # Qui-Quadrado e p-valor
        absolute_table = pd.crosstab(df_temp[v1], df_temp[v2])
        try:
            chi2, p_val, _, _ = chi2_contingency(absolute_table)
        except:
            p_val = 1.0

        # Filtro de p-valor
        if p_value_only and (np.isnan(p_val) or p_val > 0.05):
            continue

        # V de Cram√©r
        n = absolute_table.values.sum()
        r, k = absolute_table.shape
        v_cramer = np.sqrt(chi2 / (n * (min(r, k) - 1))) if n > 0 else 0

        # Benchmark de For√ßa
        if v_cramer <= 0.10: interp = "Muito Fraca"
        elif v_cramer <= 0.19: interp = "Fraca"
        elif v_cramer <= 0.29: interp = "Moderada"
        elif v_cramer <= 0.49: interp = "Forte"
        else: interp = "Muito Forte"

        try:
            kappa = cohen_kappa_score(df_temp[v1], df_temp[v2])
        except:
            kappa = np.nan

        ranking_quali.append({
            'Vari√°vel 1': v1,
            'Vari√°vel 2': v2,
            'V de Cram√©r': round(v_cramer, 4),
            'For√ßa': interp,
            'p-valor': round(p_val, 4),
            'Kappa': round(kappa, 4) if not np.isnan(kappa) else "N/A"
        })

        # --- 4. GERA√á√ÉO DE GR√ÅFICOS ---
        if show_graphs:
            plot_table = pd.crosstab(df_temp[v2], df_temp[v1],
                                     normalize=normalize if normalize != 'none' else False)

            # Limita o tamanho do heatmap se houver muitas categorias para evitar travamentos
            if df_temp[v1].nunique() <= 20 and df_temp[v2].nunique() <= 20:
                fig = px.imshow(
                    plot_table,
                    labels=dict(x=v1, y=v2, color='Frequ√™ncia'),
                    text_auto=".2f" if normalize != 'none' else True,
                    aspect="auto",
                    color_continuous_scale="Viridis",
                    title=f"ASSOCIA√á√ÉO SIGNIFICATIVA: {v1} vs {v2}<br><sup>V de Cram√©r: {v_cramer:.2f} ({interp}) | p-valor: {p_val:.4f}</sup>"
                )

                fig.update_layout(width=600, height=450, margin=dict(t=100))
                fig.update_xaxes(tickfont=dict(size=9))
                fig.update_yaxes(tickfont=dict(size=9))
                fig.show()

    # --- 5. RANKING FINAL ---
    if ranking_quali:
        df_ranking = pd.DataFrame(ranking_quali).sort_values(by='V de Cram√©r', ascending=False).reset_index(drop=True)
        print("\n" + "="*95)
        print(f"RANKING QUALITATIVO (Incluindo Datas | Filtro p-valor <= 0.05: {p_value_only})")
        print("="*95)
        print(df_ranking.to_string(index=False))
        #return df_ranking
    else:
        print("\nNenhuma associa√ß√£o qualitativa significativa encontrada.")

def univariada(df):

    for nome_coluna in df.columns:
        coluna = df[nome_coluna]

        print("="*60)
        print(f"ANALISANDO VARI√ÅVEL: {nome_coluna}")
        print("="*60)

        # 1. TRATAMENTO PARA QUALITATIVAS (Object, Category ou Booleana)
        if coluna.dtype == 'object' or isinstance(coluna.dtype, pd.CategoricalDtype) or coluna.dtype == 'bool':
            # Tabela de frequ√™ncia
            tabela = coluna.value_counts(dropna=False).reset_index()
            tabela.columns = [nome_coluna, 'frequencia_absoluta']

            tabela['frequencia_relativa'] = tabela['frequencia_absoluta'] / tabela['frequencia_absoluta'].sum()
            tabela['frequencia_acumulada'] = tabela['frequencia_relativa'].cumsum()

            print('TABELA DE FREQU√äNCIA')
            print(tabela.to_string(index=False))
            print(f'\nValores Nulos: {coluna.isna().sum()}')
            print('-' * 30)

            # Gr√°ficos Qualitativos
            if coluna.nunique() > 3:
                print('GR√ÅFICO DE BARRAS')
                plt.figure(figsize=(8, 4))
                # Ordenando barras pela frequ√™ncia
                sns.barplot(data=tabela, x='frequencia_relativa', y=nome_coluna, palette='viridis', edgecolor='black')
                plt.title(f'Distribui√ß√£o: {nome_coluna}')
            else:
                print('GR√ÅFICO DE PIZZA')
                plt.figure(figsize=(4, 4))
                plt.pie(tabela['frequencia_absoluta'], labels=tabela[nome_coluna], autopct='%1.1f%%', startangle=140, colors=sns.color_palette('pastel'))
                plt.title(f'Propor√ß√£o: {nome_coluna}')

            plt.tight_layout()
            plt.show()

        # 2. TRATAMENTO PARA QUANTITATIVAS (Num√©ricas)
        else:
            print('MEDIDAS ESTAT√çSTICAS')
            print(coluna.describe())
            print(f'\nValores Nulos: {coluna.isna().sum()}')
            print('-' * 30)

            print('HISTOGRAMA E BOXPLOT')
            # Regra de Sturges para Bins
            if len(coluna.dropna()) > 0:
                num_bins = 1 + int(math.log2(len(coluna.dropna())))

                fig, axes = plt.subplots(1, 2, figsize=(10, 4))

                # Histograma com KDE (opcional, mas ajuda a ver a forma)
                sns.histplot(x=coluna.dropna(), bins=num_bins, kde=True, ax=axes[0], color='skyblue')
                axes[0].set_title(f'Histograma: {nome_coluna}')

                # Boxplot
                sns.boxplot(y=coluna.dropna(), ax=axes[1], color='salmon')
                axes[1].set_title(f'Boxplot: {nome_coluna}')

                plt.tight_layout()
                plt.show()
            else:
                print("Vari√°vel num√©rica sem dados v√°lidos para plotagem.")

        print("\n" + " " * 60 + "\n") # Espa√ßo entre vari√°veis


def analise_temporal_automatica(
    df,
    coluna_data,
    nivel="ano",  # "ano", "ano_mes", "ano_mes_dia"
    rolling_window=3
):
    
    df = df.copy()
    df[coluna_data] = pd.to_datetime(df[coluna_data])
    
    # =====================================
    # üîé Identificar colunas num√©ricas
    # =====================================
    
    colunas_numericas = df.select_dtypes(include=np.number).columns.tolist()
    
    if not colunas_numericas:
        raise ValueError("Nenhuma coluna num√©rica encontrada no DataFrame.")
    
    print(f"\nColunas num√©ricas identificadas: {colunas_numericas}\n")
    
    # =====================================
    # üîÅ Loop para cada vari√°vel num√©rica
    # =====================================
    
    for coluna_valor in colunas_numericas:
        
        print("\n" + "#"*80)
        print(f"AN√ÅLISE DA VARI√ÅVEL: {coluna_valor.upper()}")
        print("#"*80)
        
        # =========================
        # 1Ô∏è‚É£ Agrega√ß√£o
        # =========================
        
        if nivel == "ano":
            df_agg = df.groupby(df[coluna_data].dt.year)[coluna_valor].sum()
            unidade = "ano"
            
        elif nivel == "ano_mes":
            df_agg = df.groupby(df[coluna_data].dt.to_period("M"))[coluna_valor].sum()
            df_agg.index = df_agg.index.to_timestamp()
            unidade = "m√™s"
            
        elif nivel == "ano_mes_dia":
            df_agg = df.groupby(df[coluna_data].dt.date)[coluna_valor].sum()
            df_agg.index = pd.to_datetime(df_agg.index)
            unidade = "dia"
            
        else:
            raise ValueError("nivel deve ser: 'ano', 'ano_mes' ou 'ano_mes_dia'")
        
        df_agg = df_agg.sort_index()
        
        serie = df_agg
        y = serie.values
        tempo = np.arange(len(serie))
        
        if len(y) < 3:
            print("S√©rie muito curta para an√°lise estat√≠stica.")
            continue
        
        # =========================
        # 2Ô∏è‚É£ Rolling mean
        # =========================
        
        y_smooth = pd.Series(y, index=serie.index).rolling(rolling_window).mean()
        
        # =========================
        # 3Ô∏è‚É£ Mann-Kendall
        # =========================
        
        if mk:
            try:
                teste_mk = mk.hamed_rao_modification_test(y)
            except:
                teste_mk = mk.original_test(y)
            resultado_mk = teste_mk.trend
            p_mk = teste_mk.p
        else:
            resultado_mk = "indispon√≠vel"
            p_mk = np.nan
        
        # =========================
        # 4Ô∏è‚É£ Regress√£o linear
        # =========================
        
        slope, intercept, r_val, p_val, std_err = stats.linregress(tempo, y)
        r_quadrado = r_val**2
        crescimento_pct = (slope / np.mean(y)) * 100 if np.mean(y) != 0 else 0
        
        # =========================
        # 5Ô∏è‚É£ ADF e Autocorrela√ß√£o
        # =========================
        
        try:
            adf_p = adfuller(y)[1]
        except:
            adf_p = np.nan
        
        try:
            autocorr_lag1 = acf(y, nlags=1)[1]
        except:
            autocorr_lag1 = np.nan
        
        # =========================
        # 6Ô∏è‚É£ Plotagem
        # =========================
        
        plt.figure(figsize=(9,4))
        sns.set_style("whitegrid")
        
        sns.lineplot(x=serie.index, y=y, marker='o', linewidth=2.5, label='S√©rie Observada')
        plt.plot(serie.index, y_smooth, linewidth=2, alpha=0.7,
                 label=f'M√©dia M√≥vel ({rolling_window})')
        
        plt.plot(serie.index,
                 intercept + slope * tempo,
                 'r--',
                 label=f'Tend√™ncia Linear (R¬≤={r_quadrado:.2f})')
        
        plt.title(f'An√°lise Temporal ({nivel}) - {coluna_valor.upper()}')
        plt.xlabel("Tempo")
        plt.ylabel(coluna_valor.capitalize())
        
        plt.legend()
        plt.tight_layout()
        plt.show()
        
        # =========================
        # 7Ô∏è‚É£ INTERPRETA√á√ÉO ANAL√çTICA (MODELO EXPANDIDO)
        # =========================
        
        print("\n" + "="*70)
        print(f"INTERPRETA√á√ÉO ESTAT√çSTICA ‚Äì {coluna_valor.upper()} ({nivel.upper()})")
        print("="*70)
        
        # 1Ô∏è‚É£ Tend√™ncia
        if not np.isnan(p_mk):
            print("\n1Ô∏è‚É£ Tend√™ncia (Mann-Kendall)")
            if p_mk < 0.05:
                print(f"O teste indica tend√™ncia estatisticamente significativa de {resultado_mk.lower()} (p={p_mk:.4f}).")
                print("Isso sugere que o comportamento da s√©rie n√£o √© aleat√≥rio, havendo dire√ß√£o consistente ao longo do tempo.")
            else:
                print(f"N√£o h√° evid√™ncia estat√≠stica suficiente de tend√™ncia (p={p_mk:.4f}).")
        
        # 2Ô∏è‚É£ Crescimento
        print("\n2Ô∏è‚É£ Magnitude do Crescimento")
        print(f"O crescimento m√©dio estimado √© de {slope:,.2f} por {unidade}.")
        print(f"Em termos relativos, isso representa aproximadamente {crescimento_pct:.2f}% ao {unidade}.")
        
        if crescimento_pct > 10:
            print("Trata-se de um crescimento estrutural elevado.")
        elif crescimento_pct > 3:
            print("O crescimento pode ser considerado moderado.")
        else:
            print("O crescimento √© relativamente baixo.")
        
        # 3Ô∏è‚É£ Ajuste da tend√™ncia
        print("\n3Ô∏è‚É£ Ajuste do Modelo Linear")
        print(f"O modelo linear apresenta R¬≤ de {r_quadrado:.2%},")
        print("indicando a propor√ß√£o da varia√ß√£o temporal explicada por uma tend√™ncia linear.")
        
        # 4Ô∏è‚É£ Estacionariedade
        print("\n4Ô∏è‚É£ Estacionariedade (Teste ADF)")
        if not np.isnan(adf_p):
            if adf_p < 0.05:
                print(f"O teste ADF indica estacionariedade (p={adf_p:.4f}).")
                print("Isso sugere que a s√©rie oscila em torno de uma m√©dia est√°vel ao longo do tempo.")
            else:
                print(f"O teste ADF n√£o rejeita a hip√≥tese de raiz unit√°ria (p={adf_p:.4f}).")
                print("Portanto, a s√©rie apresenta comportamento n√£o estacion√°rio,")
                print("compat√≠vel com tend√™ncia estrutural ou crescimento persistente.")
        
        # 5Ô∏è‚É£ Depend√™ncia temporal
        print("\n5Ô∏è‚É£ Depend√™ncia Temporal")
        if not np.isnan(autocorr_lag1):
            print(f"A autocorrela√ß√£o de primeira ordem √© {autocorr_lag1:.2f}.")
            
            if autocorr_lag1 > 0.7:
                print("O valor √© elevado, indicando forte persist√™ncia temporal.")
                print("Isso significa que os valores atuais dependem fortemente do per√≠odo anterior.")
            elif autocorr_lag1 > 0.4:
                print("H√° persist√™ncia temporal moderada entre per√≠odos consecutivos.")
            else:
                print("A depend√™ncia temporal √© baixa.")
        
        # 6Ô∏è‚É£ Diagn√≥stico Final
        print("\nConclus√£o Geral")
        
        if p_mk < 0.05 and crescimento_pct > 5:
            print("Os resultados apontam evid√™ncia robusta de crescimento estrutural consistente.")
        
        if adf_p > 0.05:
            print("A s√©rie apresenta comportamento n√£o estacion√°rio, sugerindo din√¢mica evolutiva ao longo do tempo.")
        
        if autocorr_lag1 > 0.7:
            print("Observa-se alta mem√≥ria temporal, caracter√≠stica de processos persistentes.")
        
        print("="*70)
